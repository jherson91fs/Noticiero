import requests
from bs4 import BeautifulSoup
from datetime import datetime
from db import guardar_noticia

# ----------------- LISTA DE FUENTES -----------------
# Atributos:
#   - url: p√°gina principal
#   - fuente: nombre del medio
#   - base: dominio (para links relativos)
#   - selector: selector CSS para noticias
#   - selector_img: selector CSS para im√°genes (opcional)
#   - categoria: categor√≠a general

FUENTES = [
    {
        "url": "https://www.bbc.com/mundo",
        "fuente": "BBC Mundo",
        "base": "https://www.bbc.com",
        "selector": "a",
        "selector_img": "img",   # ejemplo gen√©rico
        "categoria": "Internacional"
    },
    {
        "url": "https://rpp.pe/",
        "fuente": "RPP Noticias",
        "base": "https://rpp.pe",
        "selector": "div.block-news a",
        "selector_img": "div.block-news img",
        "categoria": "Nacional"
    },
    {
        "url": "https://elcomercio.pe/",
        "fuente": "El Comercio",
        "base": "https://elcomercio.pe",
        "selector": "h2 a",
        "selector_img": "figure img",
        "categoria": "Nacional"
    },
    {
        "url": "https://peru21.pe/",
        "fuente": "Per√∫21",
        "base": "https://peru21.pe",
        "selector": "h2 a",
        "selector_img": "figure img",
        "categoria": "Nacional"
    },
    {
        "url": "https://larepublica.pe/",
        "fuente": "La Rep√∫blica",
        "base": "https://larepublica.pe",
        "selector": "h2 a",
        "selector_img": "picture img",
        "categoria": "Nacional"
    }
]

# ----------------- FUNCI√ìN GEN√âRICA -----------------
def scrape_fuente(fuente):
    print(f"üåê Scrapeando {fuente['fuente']}...")

    try:
        response = requests.get(fuente["url"], timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")

        articulos = soup.select(fuente["selector"])
        imagenes = soup.select(fuente.get("selector_img", "")) if fuente.get("selector_img") else []

        for i, articulo in enumerate(articulos):
            titulo = articulo.get_text(strip=True)
            link = articulo.get("href")

            if not titulo or not link:
                continue
            if not link.startswith("http"):
                link = fuente["base"] + link

            # Imagen asociada (si existe)
            imagen = None
            if i < len(imagenes):
                img_tag = imagenes[i]
                if img_tag.has_attr("src"):
                    imagen = img_tag["src"]
                elif img_tag.has_attr("data-src"):
                    imagen = img_tag["data-src"]

                # Completar si es relativa
                if imagen and not imagen.startswith("http"):
                    imagen = fuente["base"] + imagen

            # Otros datos
            categoria = fuente["categoria"]
            fecha = datetime.today().date()
            resumen = articulo.get("title") if articulo.has_attr("title") else None
            autor = None

            guardar_noticia(
                titulo, link, categoria, fecha, resumen, autor, imagen, fuente["fuente"]
            )

    except Exception as e:
        print(f"‚ùå Error en {fuente['fuente']}: {e}")

# ----------------- MAIN -----------------
if __name__ == "__main__":
    print("üöÄ Iniciando scraping multip√°ginas con im√°genes...")
    for fuente in FUENTES:
        scrape_fuente(fuente)
    print("‚úÖ Finaliz√≥ scraping de todas las fuentes.")
